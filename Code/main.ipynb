import cv2
import os
import shutil
import numpy as np
from ultralytics import YOLO
import easyocr
import mysql.connector
from datetime import datetime
import openai
import gradio as gr
from datasets import load_dataset
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

# -----------------------------
# CONFIGURATION
# -----------------------------
openai.api_key = "api-key"
HELMET_DATASET_PATH = "archive/helmet_dataset"
INDIAN_PLATES_PATH = "archive (1)/indian_plates_dataset"
VIDEO_PATH = "archive (2)/video/test_video.mp4"
VIDEO_FRAMES_PATH = "archive2/video_frames"
os.makedirs(VIDEO_FRAMES_PATH, exist_ok=True)

# -----------------------------
# 1. Load BDD100K Dataset
# -----------------------------
print("Loading BDD100K dataset...")
ds = load_dataset("dgural/bdd100k")

BDD_PATH = "bdd100k_dataset"
os.makedirs(f"{BDD_PATH}/images/train", exist_ok=True)
os.makedirs(f"{BDD_PATH}/labels/train", exist_ok=True)

print("Saving BDD100K images and labels locally...")
for item in ds["train"]:
    image_path = os.path.join(BDD_PATH, "images/train", os.path.basename(item["image"].filename))
    shutil.copy(item["image"].filename, image_path)

    label_path = image_path.replace("images/train", "labels/train").replace(".jpg", ".txt")
    with open(label_path, "w") as f:
        for obj in item["labels"]:
            cls_id = obj["category_id"]  # YOLO format
            bbox = obj["bbox"]
            f.write(f"{cls_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\n")

print("BDD100K dataset ready.")

# -----------------------------
# 2. Extract Frames from Video Dataset
# -----------------------------
cap = cv2.VideoCapture(VIDEO_PATH)
frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    if frame_count % 5 == 0:
        cv2.imwrite(os.path.join(VIDEO_FRAMES_PATH, f"frame_{frame_count}.jpg"), frame)
    frame_count += 1
cap.release()
print(f"Extracted {frame_count} frames from video.")

# -----------------------------
# 3. YOLO Training YAML
# -----------------------------
multi_dataset_yaml = f"""
train:
  - {HELMET_DATASET_PATH}/images
  - {INDIAN_PLATES_PATH}/Indian_Number_Plates
  - {VIDEO_FRAMES_PATH}
  - {BDD_PATH}/images/train
val:
  - {HELMET_DATASET_PATH}/images_val
  - {INDIAN_PLATES_PATH}/Indian_Number_Plates_val
nc: 4
names: ['person', 'helmet', 'vehicle', 'number_plate']
"""

with open("multi_dataset.yaml", "w") as f:
    f.write(multi_dataset_yaml)

# Uncomment to train YOLO model (requires GPU)
os.system("yolo task=detect mode=train model=yolov8n.pt data=multi_dataset.yaml epochs=50 imgsz=640")

# -----------------------------
# 4. Load YOLO Model
# -----------------------------
yolo_model = YOLO("runs/detect/train/weights/best.pt")  # trained model path

# -----------------------------
# 5. OCR Setup
# -----------------------------
ocr_reader = easyocr.Reader(['en'])

def detect_violations(frame):
    results = yolo_model(frame)[0]
    violations = []
    for box, cls, conf in zip(results.boxes.xyxy, results.boxes.cls, results.boxes.conf):
        x1, y1, x2, y2 = map(int, box)
        class_name = results.names[int(cls)]
        if class_name == 'person':
            violations.append({"type": "Helmet Violation", "bbox": (x1, y1, x2, y2)})
        if class_name == 'vehicle':
            violations.append({"type": "Red Light Violation", "bbox": (x1, y1, x2, y2)})
        if class_name == 'number_plate':
            violations.append({"type": "License Plate Violation", "bbox": (x1, y1, x2, y2)})
    return violations, results

def extract_vehicle_number(frame, bbox):
    x1, y1, x2, y2 = bbox
    vehicle_crop = frame[y1:y2, x1:x2]
    result = ocr_reader.readtext(vehicle_crop)
    vehicle_number = "".join([r[1] for r in result])
    return vehicle_number

# -----------------------------
# 6. Report Generation
# -----------------------------
def generate_report(vehicle_number, violation_type, timestamp, location):
    prompt = f"""
    Generate a human-readable legal traffic violation report:
    Vehicle: {vehicle_number}
    Violation: {violation_type}
    Timestamp: {timestamp}
    Location: {location}
    """
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=150
    )
    return response['choices'][0]['message']['content']

# -----------------------------
# 7. Database Setup
# -----------------------------
db = mysql.connector.connect(
    host="localhost",
    user="root",
    password="SpadeZ1027#",  # <-- Change this to your MySQL password
    database="traffic_violation"
)
cursor = db.cursor()
cursor.execute("""
CREATE TABLE IF NOT EXISTS violation_reports (
    id INT AUTO_INCREMENT PRIMARY KEY,
    vehicle_number VARCHAR(20),
    violation_type VARCHAR(50),
    timestamp DATETIME,
    location VARCHAR(100),
    report_text TEXT
)
""")
db.commit()

def save_report(vehicle_number, violation_type, timestamp, location, report_text):
    cursor.execute("""
        INSERT INTO violation_reports (vehicle_number, violation_type, timestamp, location, report_text)
        VALUES (%s, %s, %s, %s, %s)
    """, (vehicle_number, violation_type, timestamp, location, report_text))
    db.commit()

# -----------------------------
# 8. Evaluation Metrics
# -----------------------------
def evaluate_system(ground_truth, predictions):
    label_map = {"helmet":0, "lane":1, "redlight":2, "none":3}
    y_true = [label_map[x] for x in ground_truth]
    y_pred = [label_map[x] for x in predictions]
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, average="weighted"),
        "Recall": recall_score(y_true, y_pred, average="weighted"),
        "F1 Score": f1_score(y_true, y_pred, average="weighted")
    }

# -----------------------------
# 9. Processing Functions
# -----------------------------
def process_video(video_path, location):
    cap = cv2.VideoCapture(video_path)
    frame_rate = 5
    frame_count = 0
    all_ground_truth = []
    all_predictions = []
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_count += 1
        if frame_count % frame_rate != 0:
            continue
        violations, _ = detect_violations(frame)
        for v in violations:
            vehicle_number = extract_vehicle_number(frame, v['bbox'])
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            report = generate_report(vehicle_number, v['type'], timestamp, location)
            save_report(vehicle_number, v['type'], timestamp, location, report)
            all_predictions.append(v['type'].lower().replace(" ", ""))
            all_ground_truth.append("helmet")  # Replace with actual GT labels
    cap.release()
    return evaluate_system(all_ground_truth, all_predictions)

def process_image(image_path, location):
    frame = cv2.imread(image_path)
    violations, _ = detect_violations(frame)
    results_list = []
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    for v in violations:
        vehicle_number = extract_vehicle_number(frame, v['bbox'])
        report = generate_report(vehicle_number, v['type'], timestamp, location)
        save_report(vehicle_number, v['type'], timestamp, location, report)
        results_list.append(report)
    return {"violations": results_list}

# -----------------------------
# 10. Gradio Interface
# -----------------------------
def run_gradio(video=None, image=None):
    location = "Sample Location"
    if video:
        video.save("temp_video.mp4")
        return process_video("temp_video.mp4", location)
    elif image:
        image.save("temp_image.jpg")
        return process_image("temp_image.jpg", location)
    return {"error": "No file uploaded"}

iface = gr.Interface(
    fn=run_gradio,
    inputs=[gr.Video(label="Upload Traffic Video"), gr.Image(label="Or Upload an Image")],
    outputs="json"
)
iface.launch()
